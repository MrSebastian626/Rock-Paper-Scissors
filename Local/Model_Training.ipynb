{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "#Resizing all of our data into 32x32 bmps so we can use that to train our model\n",
    "def resize_images(input_dir, output_dir, size=(32, 32)):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for class_name in os.listdir(input_dir):\n",
    "        class_dir = os.path.join(input_dir, class_name)\n",
    "        output_class_dir = os.path.join(output_dir, class_name)\n",
    "        if not os.path.exists(output_class_dir):\n",
    "            os.makedirs(output_class_dir)\n",
    "\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "            img_resized = img.resize(size)\n",
    "            img_resized.save(os.path.join(output_class_dir, img_name))\n",
    "\n",
    "# Example usage for my specific case:\n",
    "resize_images('dataset_final', 'dataset_resized', size=(32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 3 classes.\n",
      "Found 219 images belonging to 3 classes.\n",
      "Train Images shape: (1712, 32, 32, 1)\n",
      "Train Labels shape: (1712, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define dataset paths, change to yours as needed\n",
    "train_path = \"dataset_resized\"\n",
    "test_path = \"dataset_test\"\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Load training data\n",
    "train_datagen = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=32,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Load testing data\n",
    "test_datagen = datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=32,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Convert training dataset to NumPy arrays\n",
    "train_images, train_labels = [], []\n",
    "\n",
    "for img_batch, label_batch in train_datagen:\n",
    "    train_images.append(img_batch)\n",
    "    train_labels.append(label_batch)\n",
    "\n",
    "    if len(train_images) * train_datagen.batch_size >= train_datagen.samples:\n",
    "        break  # Stop after processing all images\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "train_images = np.concatenate(train_images, axis=0)\n",
    "train_labels = np.concatenate(train_labels, axis=0)\n",
    "\n",
    "# Convert lists to single NumPy arrays\n",
    "train_images = np.array(train_images) \n",
    "train_labels = np.array(train_labels)  \n",
    "\n",
    "\n",
    "# Print dataset shapes to double check that it was done correctly\n",
    "print(f\"Train Images shape: {train_images.shape}\")  # (num_samples, 32, 32, 1)\n",
    "print(f\"Train Labels shape: {train_labels.shape}\")  # (num_samples, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images shape: (1455, 32, 32, 1)\n",
      "Train Labels shape: (1455, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle dataset\n",
    "indices = np.arange(train_images.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "images, labels = train_images[indices], train_labels[indices]\n",
    "\n",
    "# Split dataset (70% train, 15% validation, 15% test)\n",
    "train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n",
    "    images, labels, test_size=0.15, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f\"Train Images shape: {train_imgs.shape}\")  # (num_samples, 32, 32, 1)\n",
    "print(f\"Train Labels shape: {train_labels.shape}\")  # (num_samples, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 15, 15, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                73792     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166659 (651.01 KB)\n",
      "Trainable params: 166659 (651.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), strides=2, activation=\"relu\", input_shape=(32, 32, 1)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), strides=2, activation=\"relu\"),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), strides=2, activation=\"relu\"),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")  # Output layer\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "# Note: strides of two were used to avoid using max pooling because .tmdl files don't support that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "54/54 [==============================] - 1s 13ms/step - loss: 1.0901 - accuracy: 0.4071 - val_loss: 1.0524 - val_accuracy: 0.6758\n",
      "Epoch 2/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.9883 - accuracy: 0.5199 - val_loss: 0.6289 - val_accuracy: 0.7215\n",
      "Epoch 3/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.7774 - accuracy: 0.6367 - val_loss: 0.3381 - val_accuracy: 0.9361\n",
      "Epoch 4/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.6478 - accuracy: 0.6951 - val_loss: 0.1821 - val_accuracy: 0.9635\n",
      "Epoch 5/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.5618 - accuracy: 0.7442 - val_loss: 0.1487 - val_accuracy: 0.9269\n",
      "Epoch 6/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.4791 - accuracy: 0.7909 - val_loss: 0.1463 - val_accuracy: 0.9498\n",
      "Epoch 7/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.4352 - accuracy: 0.8096 - val_loss: 0.1433 - val_accuracy: 0.9315\n",
      "Epoch 8/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.3718 - accuracy: 0.8499 - val_loss: 0.0842 - val_accuracy: 0.9680\n",
      "Epoch 9/15\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 0.3080 - accuracy: 0.8732 - val_loss: 0.0644 - val_accuracy: 0.9726\n",
      "Epoch 10/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.2251 - accuracy: 0.9235 - val_loss: 0.0593 - val_accuracy: 0.9772\n",
      "Epoch 11/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.1873 - accuracy: 0.9328 - val_loss: 0.0234 - val_accuracy: 0.9954\n",
      "Epoch 12/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.1490 - accuracy: 0.9480 - val_loss: 0.0315 - val_accuracy: 0.9863\n",
      "Epoch 13/15\n",
      "54/54 [==============================] - 1s 11ms/step - loss: 0.1319 - accuracy: 0.9550 - val_loss: 0.0254 - val_accuracy: 0.9909\n",
      "Epoch 14/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.1132 - accuracy: 0.9650 - val_loss: 0.0136 - val_accuracy: 0.9954\n",
      "Epoch 15/15\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.0916 - accuracy: 0.9679 - val_loss: 0.0146 - val_accuracy: 0.9909\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0146 - accuracy: 0.9909\n",
      "\n",
      "Test Accuracy: 0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Srive\\miniconda3\\envs\\hello\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model -- quick and dirty shortcut was to use the testing data as validation data, but validation set could be used instead\n",
    "history = model.fit(\n",
    "    train_datagen,\n",
    "    validation_data=test_datagen,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_datagen)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"rock_paper_scissors_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to convert our h5 file into a .tmdl file\n",
    "!python TinyMaix/tools/h5_to_tflite.py rock_paper_scissors_cnn.h5 model.tflite 0\n",
    "!python TinyMaix/tools/tflite2tmdl.py model.tflite model.tmdl fp32 1 32,32,1 3 0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
